{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08289bae-5e13-4184-ab61-a60ddb1d9395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, FloatType, StructType, StructField, StringType, LongType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "from libs.logger import log_execution\n",
    "\n",
    "# --- 1. Environment Configuration and Config Loading ---\n",
    "\n",
    "# Environment variable (assumed to be set by Job or widget)\n",
    "try:\n",
    "    ENV = dbutils.widgets.get(\"env_name\")\n",
    "except Exception:\n",
    "    ENV = 'TEST' # Default environment\n",
    "\n",
    "# Load YAML Configuration\n",
    "try:\n",
    "    # Adjust '../../config/config.yaml' path to the actual location if needed\n",
    "    with open('../../config/config.yaml', 'r') as file:\n",
    "        full_config = yaml.safe_load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'config.yaml' file not found! Check the path.\")\n",
    "    raise\n",
    "\n",
    "CFG = full_config.get(ENV)\n",
    "if not CFG:\n",
    "    raise ValueError(f\"Configuration not found for environment: {ENV} in YAML file.\")\n",
    "\n",
    "# --- 2. Data Path Definition ---\n",
    "catalog_name = CFG['catalog_name']\n",
    "schema_name = CFG['schema_name']\n",
    "volume_name = CFG['volume_name']\n",
    "\n",
    "# Precision constant used in the Silver layer\n",
    "PRECISION = 4 \n",
    "\n",
    "silver_table_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_silver_data\"\n",
    "gold_table_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_gold_data\"\n",
    "LOGS_PATH = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/gold_execution_logs/\"\n",
    "\n",
    "print(f\"Source Path (Silver): {silver_table_path}\")\n",
    "print(f\"Target Path (Gold): {gold_table_path}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f28b30d6-4edb-40d3-b594-27c4952d465b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. Process Initialization ---\n",
    "try:\n",
    "    print(f\"--- STARTING GOLD LAYER PROCESS (Mode: Monthly Aggregation) ---\")\n",
    "    log_execution(spark, \"03_GOLD_TRANSFORMATION\", \"STARTED\", LOGS_PATH)\n",
    "\n",
    "    # --- 4. Read Silver ---\n",
    "    df_silver = spark.read.format(\"delta\").load(silver_table_path)\n",
    "\n",
    "    # --- 5. Transformation (Business Logic) ---\n",
    "    df_gold = (\n",
    "        df_silver\n",
    "        .groupBy(\"Ticket\", \"company_name\", \"Year\", \"Month\")\n",
    "        .agg(\n",
    "            F.max(F.col(\"Close\")).alias(\"Monthly_Max_Close\"),\n",
    "            F.min(F.col(\"Close\")).alias(\"Monthly_Min_Close\"),\n",
    "            F.round(F.avg(F.col(\"Volume\")), 0).cast(LongType()).alias(\"Monthly_Avg_Volume\"),\n",
    "            F.sum(F.col(\"Volume\")).alias(\"Monthly_Total_Volume\"),\n",
    "            F.round(F.avg(F.col(\"Daily_Return_Pct\")), PRECISION).cast(FloatType()).alias(\"Monthly_Avg_Daily_Return_Pct\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- 6. Write to Gold with Optimization ---\n",
    "    table_exists = os.path.exists(f\"{gold_table_path}/_delta_log\")\n",
    "\n",
    "    (\n",
    "        df_gold.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"Ticket\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .save(gold_table_path)\n",
    "    )\n",
    "\n",
    "    # --- 7. Post-Write: Constraints & Optimization ---\n",
    "    \n",
    "    if not table_exists:\n",
    "        print(\"Applying Quality Constraints to Gold Table...\")\n",
    "        spark.sql(f\"ALTER TABLE delta.`{gold_table_path}` ADD CONSTRAINT gold_month_check CHECK (Month >= 1 AND Month <= 12)\")\n",
    "        spark.sql(f\"ALTER TABLE delta.`{gold_table_path}` ADD CONSTRAINT gold_price_check CHECK (Monthly_Max_Close >= Monthly_Min_Close)\")\n",
    "        spark.sql(f\"ALTER TABLE delta.`{gold_table_path}` ADD CONSTRAINT gold_ticket_not_null CHECK (Ticket IS NOT NULL)\")\n",
    "\n",
    "    # Optimization\n",
    "    print(\"Optimizing Gold table (Z-ORDER)...\")\n",
    "    spark.sql(f\"OPTIMIZE delta.`{gold_table_path}` ZORDER BY (Year, Month)\")\n",
    "\n",
    "    #VACUUM\n",
    "    print(\"Running VACUUM...\")\n",
    "    spark.sql(f\"VACUUM delta.`{gold_table_path}` RETAIN 168 HOURS\")\n",
    "\n",
    "    log_execution(spark, \"03_GOLD_TRANSFORMATION\", \"SUCCESS\", LOGS_PATH)\n",
    "    print(f\"âœ… SUCCESS: Gold layer processing finished.\")\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = str(e)[:500]\n",
    "    log_execution(spark, \"03_GOLD_TRANSFORMATION\", \"FAILED\", LOGS_PATH, message=error_msg)\n",
    "    print(f\"FATAL ERROR in Gold process: {e}\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Yfinance-Gold-layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
