{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec1190c0-d9ce-459c-8c23-6297afb9bc44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ../../requirements.txt\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0eb81c4-436c-40f0-b971-3c1db1e2e6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, TimestampType\n",
    "import yaml\n",
    "from pyspark.sql.functions import to_date, col\n",
    "import sys\n",
    "\n",
    "# Adding the project root folder to sys.path so that imports work\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "from tests.bronze_tests import verify_bronze_batch\n",
    "\n",
    "\n",
    "# --- DELTA LAKE IMPORT ---\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import types as T\n",
    "#logger\n",
    "from libs.logger import log_execution\n",
    "\n",
    "# --- 1. Environment Configuration ---\n",
    "try:\n",
    "    # dbutils.widgets.get() is the standard method for reading Job parameters\n",
    "    ENV = dbutils.widgets.get(\"env_name\") \n",
    "    print(f\"Environment configured by Job (ENV): {ENV}\")\n",
    "except Exception:\n",
    "    # Fallback for interactive/manual execution\n",
    "    ENV = 'TEST' \n",
    "    print(f\"Interactive/Manual execution. Using default ENV: {ENV}\")\n",
    "\n",
    "# --- 2. Load Configuration ---\n",
    "try:\n",
    "    # Adjust path to config.yaml if necessary\n",
    "    with open('../../config/config.yaml', 'r') as file:\n",
    "        full_config = yaml.safe_load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'config.yaml' file not found! Check the path.\")\n",
    "    raise\n",
    "\n",
    "CFG = full_config.get(ENV)\n",
    "if not CFG:\n",
    "    raise ValueError(f\"Configuration not found for environment: {ENV} in YAML file.\")\n",
    "\n",
    "# Define key configuration variables\n",
    "catalog_name = CFG['catalog_name']\n",
    "schema_name = CFG['schema_name']\n",
    "volume_name = CFG['volume_name']\n",
    "sql_table = CFG['sql_table_name']\n",
    "LOGS_PATH = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/bronze_execution_logs/\"\n",
    "BRONZE_TABLE_PATH = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_bronze_data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ee8666-c857-4324-a2ae-05a3ff98c03d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. Get Tickers from Databricks SQL Table ---\n",
    "try:\n",
    "    sql_query = f\"SELECT Ticket, company_name FROM {sql_table}\"\n",
    "    spark_df_tickets = spark.sql(sql_query)\n",
    "    df_tickets_list = spark_df_tickets.collect()\n",
    "    company_info_dict = {row['Ticket']: row['company_name'] for row in df_tickets_list}\n",
    "    gpw_ticket_list = list(company_info_dict.keys())\n",
    "    print(\"Successfully created Spark DataFrame from SQL table.\")\n",
    "    print(\"Tickers to process:\", gpw_ticket_list)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from SQL table: {e}.\")\n",
    "    gpw_ticket_list = []\n",
    "    company_info_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f0b22f-3005-49f4-864a-c287022f0f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the base directory if it doesn't exist\n",
    "try:\n",
    "    dbutils.fs.mkdirs(BRONZE_TABLE_PATH)\n",
    "    print(f\"Successfully created directory for the SINGLE BRONZE TABLE: {BRONZE_TABLE_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Directory already exists or an error occurred during creation: {e}\")\n",
    "\n",
    "# Set end date for yfinance download\n",
    "end_date = date.today()\n",
    "\n",
    "if not gpw_ticket_list:\n",
    "    print(\"No company data to process. Using fallback data for demonstration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac5549e9-4cb4-4522-9727-2584c21852e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Helper function to check if a path is a Delta Table ---\n",
    "def is_delta_table(path):\n",
    "    \"\"\"Checks if the given path contains a _delta_log folder.\"\"\"\n",
    "    try:\n",
    "        dbutils.fs.ls(os.path.join(path, \"_delta_log\")) \n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# --- Schema Definition ---\n",
    "bronze_schema = T.StructType([\n",
    "    T.StructField(\"Date\", T.DateType(), True),\n",
    "    T.StructField(\"Open\", T.DoubleType(), True),\n",
    "    T.StructField(\"High\", T.DoubleType(), True),\n",
    "    T.StructField(\"Low\", T.DoubleType(), True),\n",
    "    T.StructField(\"Close\", T.DoubleType(), True),\n",
    "    T.StructField(\"Volume\", T.LongType(), True),\n",
    "    T.StructField(\"Ticket\", T.StringType(), True),\n",
    "    T.StructField(\"company_name\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "try:\n",
    "    print(f\"Starting Bronze Ingestion for environment: {ENV}\")\n",
    "    log_execution(spark, \"01_BRONZE_INGESTION\", \"STARTED\", LOGS_PATH)\n",
    "\n",
    "    is_full_table_existing = is_delta_table(BRONZE_TABLE_PATH)\n",
    "    all_new_data_list = []\n",
    "\n",
    "    # --- 1. OPTIMIZED METADATA FETCHING ---\n",
    "    last_dates_dict = {}\n",
    "    if is_full_table_existing:\n",
    "        try:\n",
    "            print(\"Fetching current state of Bronze Table...\")\n",
    "            max_dates_df = spark.read.format(\"delta\").load(BRONZE_TABLE_PATH) \\\n",
    "                                .groupBy(\"Ticket\") \\\n",
    "                                .agg(F.max(F.col(\"Date\").cast(\"date\")).alias(\"max_date\")) \\\n",
    "                                .collect()\n",
    "            last_dates_dict = {row['Ticket']: row['max_date'] for row in max_dates_df}\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Metadata fetch failed: {e}\")\n",
    "\n",
    "    print(\"\\n--- PHASE 1: COLLECTING DATA ---\")\n",
    "\n",
    "    for t in gpw_ticket_list:\n",
    "        yfinance_ticker = f\"{t}.WA\"\n",
    "        last_download_date = last_dates_dict.get(t)\n",
    "        start_date_for_update = None\n",
    "\n",
    "        if is_full_table_existing and last_download_date:\n",
    "            if last_download_date < date.today():\n",
    "                start_date_for_update = (pd.to_datetime(last_download_date) + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "            else:\n",
    "                continue \n",
    "        \n",
    "        try:\n",
    "            # Download data\n",
    "            if not is_full_table_existing or not last_download_date:\n",
    "                data = yf.download(yfinance_ticker, period='max', end=end_date)\n",
    "            else:\n",
    "                data = yf.download(yfinance_ticker, start=start_date_for_update, end=end_date)\n",
    "            \n",
    "            if data is not None and not data.empty:\n",
    "                # FIX for InvalidIndexError: Remove duplicate indices if they exist\n",
    "                data = data[~data.index.duplicated(keep='last')]\n",
    "                \n",
    "                df_tmp = data.reset_index()\n",
    "                \n",
    "                # Clean MultiIndex columns if present\n",
    "                if isinstance(df_tmp.columns, pd.MultiIndex):\n",
    "                    df_tmp.columns = df_tmp.columns.map(lambda x: x[0] if isinstance(x, tuple) else x)\n",
    "                \n",
    "                # Final check: Ensure \"Date\" column is unique before adding it to the list\n",
    "                df_tmp = df_tmp.loc[:, ~df_tmp.columns.duplicated()]\n",
    "\n",
    "                # Add Metadata\n",
    "                df_tmp['Ticket'] = t\n",
    "                df_tmp['company_name'] = company_info_dict.get(t, 'N/A')\n",
    "                df_tmp['Date'] = pd.to_datetime(df_tmp['Date']).dt.date\n",
    "                \n",
    "                # Use only columns defined in schema to avoid \"Extra column\" errors during concat\n",
    "                cols_to_keep = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticket', 'company_name']\n",
    "                df_tmp = df_tmp[[c for c in cols_to_keep if c in df_tmp.columns]]\n",
    "                \n",
    "                all_new_data_list.append(df_tmp)\n",
    "                print(f\"Collected {t}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {t}: {e}\")\n",
    "\n",
    "    # --- 2. PHASE 2: BATCH MERGE ---\n",
    "    if all_new_data_list:\n",
    "        print(\"\\n--- PHASE 2: BATCH MERGE ---\")\n",
    "        \n",
    "        # Combined DataFrame with unique columns\n",
    "        combined_pandas_df = pd.concat(all_new_data_list, ignore_index=True)\n",
    "        \n",
    "        # Safety: Final check for duplicates in the combined batch\n",
    "        combined_pandas_df = combined_pandas_df.drop_duplicates(subset=['Date', 'Ticket'])\n",
    "        \n",
    "        spark_batch_df = spark.createDataFrame(combined_pandas_df, schema=bronze_schema)\n",
    "        \n",
    "        if not is_full_table_existing:\n",
    "            spark_batch_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"Ticket\").save(BRONZE_TABLE_PATH)\n",
    "            # Constraints\n",
    "            spark.sql(f\"ALTER TABLE delta.`{BRONZE_TABLE_PATH}` ADD CONSTRAINT date_not_null CHECK (Date IS NOT NULL)\")\n",
    "            spark.sql(f\"ALTER TABLE delta.`{BRONZE_TABLE_PATH}` ADD CONSTRAINT ticket_not_null CHECK (Ticket IS NOT NULL)\")\n",
    "        else:\n",
    "            deltaTable = DeltaTable.forPath(spark, BRONZE_TABLE_PATH)\n",
    "            deltaTable.alias(\"target\").merge(\n",
    "                source=spark_batch_df.alias(\"source\"),\n",
    "                condition=\"target.Date = source.Date AND target.Ticket = source.Ticket\"\n",
    "            ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "            \n",
    "        print(f\"Successfully merged {len(all_new_data_list)} tickers in one transaction.\")\n",
    "    else:\n",
    "        print(\"Nothing to update.\")\n",
    "\n",
    "    log_execution(spark, \"01_BRONZE_INGESTION\", \"SUCCESS\", LOGS_PATH)\n",
    "\n",
    "except Exception as e:\n",
    "    log_execution(spark, \"01_BRONZE_INGESTION\", \"FAILED\", LOGS_PATH, message=str(e)[:500])\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666ecc64-6c91-45ce-9a3e-30c201e845b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingest",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
