{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec1190c0-d9ce-459c-8c23-6297afb9bc44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install yfinance\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "32ee8666-c857-4324-a2ae-05a3ff98c03d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, TimestampType\n",
    "\n",
    "# Definicja sesji Spark (dla środowisk poza Databricks)\n",
    "# Spark w Databricks jest dostępny automatycznie.\n",
    "# spark = SparkSession.builder.appName(\"YfinanceDownloader\").getOrCreate()\n",
    "\n",
    "# --- KROK 1: Tworzenie df_tickets z tabeli Databricks SQL ---\n",
    "try:\n",
    "    sql_query = \"SELECT Ticket, company_name FROM google_drive.gpw_data\"\n",
    "    spark_df_tickets = spark.sql(sql_query)\n",
    "    df_tickets_list = spark_df_tickets.collect()\n",
    "    company_info_dict = {row['Ticket']: row['company_name'] for row in df_tickets_list}\n",
    "    gpw_ticket_list = list(company_info_dict.keys())\n",
    "    print(\"Pomyślnie utworzono Spark DataFrame z tabeli SQL.\")\n",
    "    print(\"Akcje do przetworzenia:\", gpw_ticket_list)\n",
    "except Exception as e:\n",
    "    print(f\"Błąd odczytu z tabeli SQL: {e}.\")\n",
    "    gpw_ticket_list = []\n",
    "    company_info_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f0b22f-3005-49f4-864a-c287022f0f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ustawienia katalogu Unity Catalog Volume\n",
    "catalog_name = \"workspace\"\n",
    "schema_name = \"default\"\n",
    "volume_name = \"yfinance_data\"\n",
    "base_output_directory = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_parquets/\"\n",
    "\n",
    "# Utworzenie katalogu bazowego, jeśli nie istnieje\n",
    "try:\n",
    "    dbutils.fs.mkdirs(base_output_directory)\n",
    "    print(f\"Pomyślnie utworzono katalog: {base_output_directory}\")\n",
    "except Exception as e:\n",
    "    print(f\"Katalog już istnieje lub wystąpił błąd podczas jego tworzenia: {e}\")\n",
    "\n",
    "# Ustawienia zakresu dat dla yfinance\n",
    "end_date = date.today()\n",
    "\n",
    "if not gpw_ticket_list:\n",
    "    print(\"Brak danych firm do przetworzenia. Używanie danych zastępczych dla demonstracji.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac5549e9-4cb4-4522-9727-2584c21852e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "print(\"\\nRozpoczynanie pobierania danych z yfinance i zapisywanie do pojedynczych plików Parquet...\")\n",
    "\n",
    "# Helper function to check if a path exists using dbutils.fs.ls\n",
    "def path_exists(path):\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "    \n",
    "# Lista kolumn, które mają być ujednolicone\n",
    "price_columns = [\"Close\", \"High\", \"Low\", \"Open\"]\n",
    "# Docelowa precyzja: 10 cyfr łącznie, 4 po przecinku (Decimal(10, 4))\n",
    "target_decimal_type = T.DecimalType(10, 4) \n",
    "\n",
    "for t in gpw_ticket_list:\n",
    "    yfinance_ticker = f\"{t}.WA\"\n",
    "    output_path = os.path.join(base_output_directory, f\"ticker={t}\")\n",
    "    print(f\"Przetwarzanie akcji: {yfinance_ticker}\")\n",
    "\n",
    "    # Sprawdzenie, czy katalog dla danego tickera już istnieje\n",
    "    if path_exists(output_path):\n",
    "        try:\n",
    "            # Plik istnieje, próbujemy odczytać i zaktualizować\n",
    "            existing_df = spark.read.parquet(output_path)\n",
    "            # Konwersja kolumny na typ DateType\n",
    "            existing_df = existing_df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "            last_download_date = existing_df.selectExpr(\"max(Date)\").collect()[0][0]\n",
    "            if last_download_date:\n",
    "                today_date_obj = date.today()\n",
    "                if last_download_date < today_date_obj:\n",
    "                    print(f\"Dane dla '{yfinance_ticker}' wymagają aktualizacji. Ostatnia data: {last_download_date}.\")\n",
    "                    start_date_for_update = (pd.to_datetime(last_download_date) + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "                    data = yf.download(yfinance_ticker, start=start_date_for_update, end=end_date)\n",
    "                    \n",
    "                    if not data.empty:\n",
    "                        df_new_data = data.reset_index()\n",
    "                        if isinstance(df_new_data.columns, pd.MultiIndex):\n",
    "                            df_new_data.columns = df_new_data.columns.map(lambda x: x[0])\n",
    "                        df_new_data.info()\n",
    "                        spark_new_data_df = spark.createDataFrame(df_new_data)\n",
    "                        spark_new_data_df = spark_new_data_df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "                        company_name = company_info_dict.get(t, 'N/A')\n",
    "                        spark_new_data_df = spark_new_data_df.withColumn(\"Ticket\", lit(t)).withColumn(\"Company_Name\", lit(company_name))\n",
    "                        \n",
    "                        combined_spark_df = existing_df.unionByName(spark_new_data_df, allowMissingColumns=True).dropDuplicates(['Date'])\n",
    "                        combined_spark_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "                        print(f\"Zaktualizowano dane dla '{yfinance_ticker}'. Dodano {spark_new_data_df.count()} nowe wiersze.\")\n",
    "                    else:\n",
    "                        print(f\"Brak nowych danych do pobrania dla '{yfinance_ticker}'.\")\n",
    "                else:\n",
    "                    print(f\"Dane dla '{yfinance_ticker}' są aktualne na dzień {last_download_date}. Pomijanie.\")\n",
    "            else:\n",
    "                print(f\"Ostrzeżenie: Kolumna 'Date' nie znaleziona lub pusta dla '{yfinance_ticker}'. Pełne ponowne pobranie.\")\n",
    "                # Jeśli kolumna Date jest pusta, traktujemy to jak brak danych\n",
    "                data = yf.download(yfinance_ticker, period='max', end=end_date)\n",
    "                \n",
    "                if not data.empty:\n",
    "                    df_new_data = data.reset_index()\n",
    "                    if isinstance(df_new_data.columns, pd.MultiIndex):\n",
    "                        df_new_data.columns = df_new_data.columns.map(lambda x: x[0])\n",
    "                    spark_new_data_df = spark.createDataFrame(df_new_data)\n",
    "                    spark_new_data_df = spark_new_data_df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "                    company_name = company_info_dict.get(t, 'N/A')\n",
    "                    spark_new_data_df = spark_new_data_df.withColumn(\"Ticket\", lit(t)).withColumn(\"Company_Name\", lit(company_name))\n",
    "                    for col_name in price_columns:\n",
    "                        spark_new_data_df = spark_new_data_df.withColumn(col_name,\n",
    "                        F.round(F.col(col_name), 4).cast(target_decimal_type))\n",
    "                    spark_new_data_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "                    print(f\"Pobrano i zapisano pełne dane dla '{yfinance_ticker}'.\")\n",
    "                else:\n",
    "                    print(f\"Brak danych dla '{yfinance_ticker}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Wystąpił błąd podczas aktualizacji dla '{yfinance_ticker}': {e}. Przystąpienie do pełnego pobierania.\")\n",
    "            \n",
    "    else:\n",
    "        # Plik nie istnieje, zaczynamy od pełnego pobierania\n",
    "        print(f\"Brak istniejących danych dla '{yfinance_ticker}'. Rozpoczęcie pełnego pobierania.\")\n",
    "        try:\n",
    "            data = yf.download(yfinance_ticker, period='max', end=end_date)\n",
    "            print(data)\n",
    "            if not data.empty:\n",
    "                df_new_data = data.reset_index()\n",
    "                if isinstance(df_new_data.columns, pd.MultiIndex):\n",
    "                    df_new_data.columns = df_new_data.columns.map(lambda x: x[0])\n",
    "                print(df_new_data)\n",
    "               #df_new_data['Date'] = df_new_data['Date'].dt.strftime('%Y-%m-%d')\n",
    "                spark_new_data_df = spark.createDataFrame(df_new_data)\n",
    "                spark_new_data_df = spark_new_data_df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "                company_name = company_info_dict.get(t, 'N/A')\n",
    "                spark_new_data_df = spark_new_data_df.withColumn(\"Ticket\", lit(t)).withColumn(\"Company_Name\", lit(company_name))\n",
    "                for col_name in price_columns:\n",
    "                    spark_new_data_df = spark_new_data_df.withColumn(col_name,\n",
    "                    F.round(F.col(col_name), 4).cast(target_decimal_type))\n",
    "                spark_new_data_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "                print(f\"Pobrano i zapisano pełne dane dla '{yfinance_ticker}'.\")\n",
    "            else:\n",
    "                print(f\"Brak danych dla '{yfinance_ticker}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Wystąpił błąd podczas pobierania lub zapisywania dla '{yfinance_ticker}': {e}\")\n",
    "            \n",
    "    time.sleep(1) # Unikaj przeciążania API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666ecc64-6c91-45ce-9a3e-30c201e845b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Yfinance download",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
