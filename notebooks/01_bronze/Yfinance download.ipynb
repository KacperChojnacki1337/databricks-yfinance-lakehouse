{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec1190c0-d9ce-459c-8c23-6297afb9bc44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ../../requirements.txt\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0eb81c4-436c-40f0-b971-3c1db1e2e6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, TimestampType\n",
    "import yaml\n",
    "\n",
    "\n",
    "try:\n",
    "    # dbutils.widgets.get() jest standardową metodą odczytu parametrów Joba\n",
    "    ENV = dbutils.widgets.get(\"env_name\") \n",
    "    print(f\"Środowisko skonfigurowane przez Job (ENV): {ENV}\")\n",
    "except Exception:\n",
    "    # Obsługa błędu - gdy uruchamiasz notebook interaktywnie lub ręcznie (spoza Joba),\n",
    "    # dbutils.widgets.get() może zgłosić błąd, bo widgety nie są zainicjowane.\n",
    "    # W takim przypadku ustawiamy wartość domyślną (np. 'TEST')\n",
    "    ENV = 'TEST' \n",
    "    print(f\"Uruchomienie interaktywne/ręczne. Użycie domyślnego ENV: {ENV}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # W Databricks, ścieżka do pliku YAML może wymagać dostosowania,\n",
    "    # jeśli nie jest on w tym samym katalogu co notatnik.\n",
    "    with open('../../config/config.yaml', 'r') as file:\n",
    "        full_config = yaml.safe_load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"BŁĄD: Plik 'config.yaml' nie został znaleziony! Sprawdź ścieżkę.\")\n",
    "    raise\n",
    "\n",
    "CFG = full_config.get(ENV)\n",
    "if not CFG:\n",
    "    raise ValueError(f\"Nie znaleziono konfiguracji dla środowiska: {ENV} w pliku YAML.\")\n",
    "\n",
    "catalog_name = CFG['catalog_name']\n",
    "schema_name = CFG['schema_name']\n",
    "volume_name = CFG['volume_name']\n",
    "sql_table = CFG['sql_table_name']\n",
    "base_output_directory = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_delta_lake_bronze/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ee8666-c857-4324-a2ae-05a3ff98c03d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- KROK 1: Tworzenie df_tickets z tabeli Databricks SQL ---\n",
    "try:\n",
    "    sql_query = f\"SELECT Ticket, company_name FROM {sql_table}\"\n",
    "    spark_df_tickets = spark.sql(sql_query)\n",
    "    df_tickets_list = spark_df_tickets.collect()\n",
    "    company_info_dict = {row['Ticket']: row['company_name'] for row in df_tickets_list}\n",
    "    gpw_ticket_list = list(company_info_dict.keys())\n",
    "    print(\"Pomyślnie utworzono Spark DataFrame z tabeli SQL.\")\n",
    "    print(\"Akcje do przetworzenia:\", gpw_ticket_list)\n",
    "except Exception as e:\n",
    "    print(f\"Błąd odczytu z tabeli SQL: {e}.\")\n",
    "    gpw_ticket_list = []\n",
    "    company_info_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f0b22f-3005-49f4-864a-c287022f0f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Utworzenie katalogu bazowego, jeśli nie istnieje\n",
    "try:\n",
    "    dbutils.fs.mkdirs(base_output_directory)\n",
    "    print(f\"Pomyślnie utworzono katalog: {base_output_directory}\")\n",
    "except Exception as e:\n",
    "    print(f\"Katalog już istnieje lub wystąpił błąd podczas jego tworzenia: {e}\")\n",
    "\n",
    "# Ustawienia zakresu dat dla yfinance\n",
    "end_date = date.today()\n",
    "\n",
    "if not gpw_ticket_list:\n",
    "    print(\"Brak danych firm do przetworzenia. Używanie danych zastępczych dla demonstracji.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac5549e9-4cb4-4522-9727-2584c21852e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "# --- DODATKOWY IMPORT DLA DELTA LAKE ---\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Helper function to check if a path is a Delta Table\n",
    "def is_delta_table(path):\n",
    "    \"\"\"Sprawdza, czy podana ścieżka zawiera folder _delta_log (czyli jest Delta Table).\"\"\"\n",
    "    try:\n",
    "        # dbutils.fs.ls zgłosi błąd, jeśli ścieżka nie istnieje.\n",
    "        dbutils.fs.ls(os.path.join(path, \"_delta_log\")) \n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"\\nRozpoczynanie pobierania danych z yfinance i zapisywanie do Delta Lake...\")\n",
    "\n",
    "for t in gpw_ticket_list:\n",
    "    yfinance_ticker = f\"{t}.WA\"\n",
    "    output_path = os.path.join(base_output_directory, f\"ticker={t}\")\n",
    "    print(f\"Przetwarzanie akcji: {yfinance_ticker}\")\n",
    "    is_existing_delta = is_delta_table(output_path)\n",
    "\n",
    "    # --- LOGIKA POBIERANIA DANYCH ---\n",
    "    start_date_for_update = None\n",
    "    if is_existing_delta:\n",
    "        try:\n",
    "            # Odczytujemy maksymalną datę z istniejącej tabeli Delta\n",
    "            delta_table = spark.read.format(\"delta\").load(output_path)\n",
    "            # Konwersja kolumny na typ DateType (jeśli nie była DateType)\n",
    "            delta_table = delta_table.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "            last_download_date = delta_table.selectExpr(\"max(Date)\").collect()[0][0]\n",
    "\n",
    "            if last_download_date and last_download_date < date.today():\n",
    "                # Ustawiamy datę początkową na dzień po ostatnio pobranym\n",
    "                start_date_for_update = (pd.to_datetime(last_download_date) + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "                print(f\"Dane dla '{yfinance_ticker}' wymagają aktualizacji od daty: {start_date_for_update}.\")\n",
    "            elif last_download_date:\n",
    "                 print(f\"Dane dla '{yfinance_ticker}' są aktualne na dzień {last_download_date}. Pomijanie.\")\n",
    "                 time.sleep(1) # Unikaj przeciążania API\n",
    "                 continue\n",
    "            else:\n",
    "                 # Jeśli last_download_date jest None (tabela pusta, ale istnieje)\n",
    "                 print(f\"Ostrzeżenie: Istniejąca tabela Delta pusta dla '{yfinance_ticker}'. Pełne ponowne pobranie.\")\n",
    "        except Exception as e:\n",
    "            # Błąd odczytu Delta Table, traktujemy jak konieczność pełnego pobierania\n",
    "            print(f\"Wystąpił błąd odczytu Delta Table dla '{yfinance_ticker}': {e}. Pełne ponowne pobranie.\")\n",
    "            is_existing_delta = False\n",
    "            \n",
    "    # Ustawienia daty początkowej dla yfinance\n",
    "    if not is_existing_delta or (is_existing_delta and not last_download_date):\n",
    "        # Pełne pobieranie od maksymalnej dostępnej daty\n",
    "        data = yf.download(yfinance_ticker, period='max', end=end_date)\n",
    "        print(f\"Brak istniejących danych Delta dla '{yfinance_ticker}'. Rozpoczęcie pełnego pobierania.\")\n",
    "    elif start_date_for_update:\n",
    "        # Pobieranie tylko nowych danych\n",
    "        data = yf.download(yfinance_ticker, start=start_date_for_update, end=end_date)\n",
    "    else:\n",
    "        # Dane aktualne (powinno być obsłużone w bloku powyżej, ale zabezpieczenie)\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "    # --- LOGIKA ZAPISU DO DELTA LAKE ---\n",
    "    if not data.empty:\n",
    "        # Przygotowanie danych do zapisu Spark DataFrame\n",
    "        df_new_data = data.reset_index()\n",
    "        # Obsługa MultiIndex po pobraniu (jeśli wystąpi)\n",
    "        if isinstance(df_new_data.columns, pd.MultiIndex):\n",
    "            df_new_data.columns = df_new_data.columns.map(lambda x: x[0])\n",
    "        # Konwersja do Spark DataFrame\n",
    "        spark_new_data_df = spark.createDataFrame(df_new_data)\n",
    "\n",
    "        # Czyszczenie i dodawanie kolumn\n",
    "        spark_new_data_df = spark_new_data_df.withColumn(\"Date\", col(\"Date\").cast(\"date\")) \\\n",
    "                                              .withColumn(\"Ticket\", lit(t)) \\\n",
    "                                              .withColumn(\"company_name\", lit(company_info_dict.get(t, 'N/A')))\n",
    "        # Upewnienie się, że DF ma wymagane kolumny\n",
    "        cols_to_select = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Ticket', 'company_name']\n",
    "        spark_new_data_df = spark_new_data_df.select([F.col(c) for c in cols_to_select if c in spark_new_data_df.columns])\n",
    "\n",
    "        # A) Jeśli tabela NIE ISTNIEJE, tworzymy nową jako Delta Lake\n",
    "        if not is_existing_delta:\n",
    "            try:\n",
    "                # Tworzymy pierwszą wersję Delta Table (partycjonowaną)\n",
    "                spark_new_data_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"Ticket\").save(output_path)\n",
    "                print(f\"Pobrano i zapisano **pełną nową** Delta Table dla '{yfinance_ticker}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"BŁĄD ZAPISU PEŁNEJ DELTA TABLE dla '{yfinance_ticker}': {e}\")\n",
    "        # B) Jeśli tabela ISTNIEJE, używamy MERGE\n",
    "        else:\n",
    "            try:\n",
    "                # Używamy MERGE INTO do transakcyjnej aktualizacji/wstawiania\n",
    "                deltaTable = DeltaTable.forPath(spark, output_path)\n",
    "                \n",
    "                deltaTable.alias(\"target\") \\\n",
    "                    .merge(\n",
    "                        source=spark_new_data_df.alias(\"source\"),\n",
    "                        # Łączymy na podstawie 'Date' i 'Ticket' (kolumny partycji)\n",
    "                        condition=\"target.Date = source.Date AND target.Ticket = source.Ticket\"\n",
    "                    ) \\\n",
    "                    .whenMatchedUpdateAll() \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "                \n",
    "                print(f\"Zaktualizowano dane dla '{yfinance_ticker}' za pomocą **MERGE INTO** Delta Lake. Dodano/zaktualizowano {spark_new_data_df.count()} wierszy.\")\n",
    "            except Exception as e:\n",
    "                print(f\"BŁĄD MERGE INTO DELTA LAKE dla '{yfinance_ticker}': {e}. Pełne ponowne pobranie mogło by pomóc.\")\n",
    "                \n",
    "    else:\n",
    "        print(f\"Brak nowych danych do pobrania dla '{yfinance_ticker}'.\")\n",
    "        \n",
    "    time.sleep(1) # Unikaj przeciążania API\n",
    "    \n",
    "print(\"\\n--- ZAKOŃCZONO PROCES AKTUALIZACJI/WSTAWIANIA DO DELTA LAKE ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666ecc64-6c91-45ce-9a3e-30c201e845b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Yfinance download",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
