{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec1190c0-d9ce-459c-8c23-6297afb9bc44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ../../requirements.txt\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0eb81c4-436c-40f0-b971-3c1db1e2e6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, TimestampType\n",
    "import yaml\n",
    "\n",
    "\n",
    "try:\n",
    "    # dbutils.widgets.get() jest standardową metodą odczytu parametrów Joba\n",
    "    ENV = dbutils.widgets.get(\"env_name\") \n",
    "    print(f\"Środowisko skonfigurowane przez Job (ENV): {ENV}\")\n",
    "except Exception:\n",
    "    # Obsługa błędu - gdy uruchamiasz notebook interaktywnie lub ręcznie (spoza Joba),\n",
    "    # dbutils.widgets.get() może zgłosić błąd, bo widgety nie są zainicjowane.\n",
    "    # W takim przypadku ustawiamy wartość domyślną (np. 'TEST')\n",
    "    ENV = 'TEST' \n",
    "    print(f\"Uruchomienie interaktywne/ręczne. Użycie domyślnego ENV: {ENV}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # W Databricks, ścieżka do pliku YAML może wymagać dostosowania,\n",
    "    # jeśli nie jest on w tym samym katalogu co notatnik.\n",
    "    with open('../../config/config.yaml', 'r') as file:\n",
    "        full_config = yaml.safe_load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"BŁĄD: Plik 'config.yaml' nie został znaleziony! Sprawdź ścieżkę.\")\n",
    "    raise\n",
    "\n",
    "CFG = full_config.get(ENV)\n",
    "if not CFG:\n",
    "    raise ValueError(f\"Nie znaleziono konfiguracji dla środowiska: {ENV} w pliku YAML.\")\n",
    "\n",
    "catalog_name = CFG['catalog_name']\n",
    "schema_name = CFG['schema_name']\n",
    "volume_name = CFG['volume_name']\n",
    "sql_table = CFG['sql_table_name']\n",
    "base_output_directory = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_parquets/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ee8666-c857-4324-a2ae-05a3ff98c03d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- KROK 1: Tworzenie df_tickets z tabeli Databricks SQL ---\n",
    "try:\n",
    "    sql_query = f\"SELECT Ticket, company_name FROM {sql_table}\"\n",
    "    spark_df_tickets = spark.sql(sql_query)\n",
    "    df_tickets_list = spark_df_tickets.collect()\n",
    "    company_info_dict = {row['Ticket']: row['company_name'] for row in df_tickets_list}\n",
    "    gpw_ticket_list = list(company_info_dict.keys())\n",
    "    print(\"Pomyślnie utworzono Spark DataFrame z tabeli SQL.\")\n",
    "    print(\"Akcje do przetworzenia:\", gpw_ticket_list)\n",
    "except Exception as e:\n",
    "    print(f\"Błąd odczytu z tabeli SQL: {e}.\")\n",
    "    gpw_ticket_list = []\n",
    "    company_info_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f0b22f-3005-49f4-864a-c287022f0f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Utworzenie katalogu bazowego, jeśli nie istnieje\n",
    "try:\n",
    "    dbutils.fs.mkdirs(base_output_directory)\n",
    "    print(f\"Pomyślnie utworzono katalog: {base_output_directory}\")\n",
    "except Exception as e:\n",
    "    print(f\"Katalog już istnieje lub wystąpił błąd podczas jego tworzenia: {e}\")\n",
    "\n",
    "# Ustawienia zakresu dat dla yfinance\n",
    "end_date = date.today()\n",
    "\n",
    "if not gpw_ticket_list:\n",
    "    print(\"Brak danych firm do przetworzenia. Używanie danych zastępczych dla demonstracji.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac5549e9-4cb4-4522-9727-2584c21852e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "print(\"\\nRozpoczynanie pobierania danych z yfinance i zapisywanie do pojedynczych plików Parquet...\")\n",
    "\n",
    "# Helper function to check if a path exists using dbutils.fs.ls\n",
    "def path_exists(path):\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "    \n",
    "\n",
    "for t in gpw_ticket_list:\n",
    "    yfinance_ticker = f\"{t}.WA\"\n",
    "    output_path = os.path.join(base_output_directory, f\"ticker={t}\")\n",
    "    print(f\"Przetwarzanie akcji: {yfinance_ticker}\")\n",
    "\n",
    "    # Sprawdzenie, czy katalog dla danego tickera już istnieje\n",
    "    if path_exists(output_path):\n",
    "        try:\n",
    "            # Plik istnieje, próbujemy odczytać i zaktualizować\n",
    "            existing_df = spark.read.parquet(output_path)\n",
    "            # Konwersja kolumny na typ DateType\n",
    "            existing_df = existing_df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "            last_download_date = existing_df.selectExpr(\"max(Date)\").collect()[0][0]\n",
    "            if last_download_date:\n",
    "                today_date_obj = date.today()\n",
    "                if last_download_date < today_date_obj:\n",
    "                    print(f\"Dane dla '{yfinance_ticker}' wymagają aktualizacji. Ostatnia data: {last_download_date}.\")\n",
    "                    start_date_for_update = (pd.to_datetime(last_download_date) + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "                    data = yf.download(yfinance_ticker, start=start_date_for_update, end=end_date)\n",
    "                    \n",
    "                    if not data.empty:\n",
    "                        df_new_data = data.reset_index()\n",
    "                        if isinstance(df_new_data.columns, pd.MultiIndex):\n",
    "                            df_new_data.columns = df_new_data.columns.map(lambda x: x[0])\n",
    "                        spark_new_data_df = spark.createDataFrame(df_new_data)\n",
    "                        spark_new_data_df = spark_new_data_df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "                        company_name = company_info_dict.get(t, 'N/A')\n",
    "                        spark_new_data_df = spark_new_data_df.withColumn(\"Ticket\", lit(t))\n",
    "                        \n",
    "                        combined_spark_df = existing_df.unionByName(spark_new_data_df, allowMissingColumns=True).dropDuplicates(['Date'])\n",
    "                        combined_spark_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "                        print(f\"Zaktualizowano dane dla '{yfinance_ticker}'. Dodano {spark_new_data_df.count()} nowe wiersze.\")\n",
    "                    else:\n",
    "                        print(f\"Brak nowych danych do pobrania dla '{yfinance_ticker}'.\")\n",
    "                else:\n",
    "                    print(f\"Dane dla '{yfinance_ticker}' są aktualne na dzień {last_download_date}. Pomijanie.\")\n",
    "            else:\n",
    "                print(f\"Ostrzeżenie: Kolumna 'Date' nie znaleziona lub pusta dla '{yfinance_ticker}'. Pełne ponowne pobranie.\")\n",
    "                # Jeśli kolumna Date jest pusta, traktujemy to jak brak danych\n",
    "                data = yf.download(yfinance_ticker, period='max', end=end_date)\n",
    "                \n",
    "                if not data.empty:\n",
    "                    df_new_data = data.reset_index()\n",
    "                    if isinstance(df_new_data.columns, pd.MultiIndex):\n",
    "                        df_new_data.columns = df_new_data.columns.map(lambda x: x[0])\n",
    "                    spark_new_data_df = spark.createDataFrame(df_new_data)\n",
    "                    spark_new_data_df = spark_new_data_df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "                    company_name = company_info_dict.get(t, 'N/A')\n",
    "                    spark_new_data_df = spark_new_data_df.withColumn(\"Ticket\", lit(t))\n",
    "                    spark_new_data_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "                    print(f\"Pobrano i zapisano pełne dane dla '{yfinance_ticker}'.\")\n",
    "                else:\n",
    "                    print(f\"Brak danych dla '{yfinance_ticker}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Wystąpił błąd podczas aktualizacji dla '{yfinance_ticker}': {e}. Przystąpienie do pełnego pobierania.\")\n",
    "            \n",
    "    else:\n",
    "        # Plik nie istnieje, zaczynamy od pełnego pobierania\n",
    "        print(f\"Brak istniejących danych dla '{yfinance_ticker}'. Rozpoczęcie pełnego pobierania.\")\n",
    "        try:\n",
    "            data = yf.download(yfinance_ticker, period='max', end=end_date)\n",
    "            if not data.empty:\n",
    "                df_new_data = data.reset_index()\n",
    "                if isinstance(df_new_data.columns, pd.MultiIndex):\n",
    "                    df_new_data.columns = df_new_data.columns.map(lambda x: x[0])\n",
    "                spark_new_data_df = spark.createDataFrame(df_new_data)\n",
    "                spark_new_data_df = spark_new_data_df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "                company_name = company_info_dict.get(t, 'N/A')\n",
    "                spark_new_data_df = spark_new_data_df.withColumn(\"Ticket\", lit(t))\n",
    "                spark_new_data_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "                print(f\"Pobrano i zapisano pełne dane dla '{yfinance_ticker}'.\")\n",
    "            else:\n",
    "                print(f\"Brak danych dla '{yfinance_ticker}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Wystąpił błąd podczas pobierania lub zapisywania dla '{yfinance_ticker}': {e}\")\n",
    "            \n",
    "    time.sleep(1) # Unikaj przeciążania API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666ecc64-6c91-45ce-9a3e-30c201e845b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Yfinance download",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
