{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec1190c0-d9ce-459c-8c23-6297afb9bc44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ../../requirements.txt\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0eb81c4-436c-40f0-b971-3c1db1e2e6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, TimestampType\n",
    "import yaml\n",
    "from pyspark.sql.functions import to_date, col\n",
    "# --- DELTA LAKE IMPORT ---\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import types as T\n",
    "#logger\n",
    "from libs.logger import log_execution\n",
    "\n",
    "# --- 1. Environment Configuration ---\n",
    "try:\n",
    "    # dbutils.widgets.get() is the standard method for reading Job parameters\n",
    "    ENV = dbutils.widgets.get(\"env_name\") \n",
    "    print(f\"Environment configured by Job (ENV): {ENV}\")\n",
    "except Exception:\n",
    "    # Fallback for interactive/manual execution\n",
    "    ENV = 'TEST' \n",
    "    print(f\"Interactive/Manual execution. Using default ENV: {ENV}\")\n",
    "\n",
    "# --- 2. Load Configuration ---\n",
    "try:\n",
    "    # Adjust path to config.yaml if necessary\n",
    "    with open('../../config/config.yaml', 'r') as file:\n",
    "        full_config = yaml.safe_load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'config.yaml' file not found! Check the path.\")\n",
    "    raise\n",
    "\n",
    "CFG = full_config.get(ENV)\n",
    "if not CFG:\n",
    "    raise ValueError(f\"Configuration not found for environment: {ENV} in YAML file.\")\n",
    "\n",
    "# Define key configuration variables\n",
    "catalog_name = CFG['catalog_name']\n",
    "schema_name = CFG['schema_name']\n",
    "volume_name = CFG['volume_name']\n",
    "sql_table = CFG['sql_table_name']\n",
    "LOGS_PATH = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/execution_logs/\"\n",
    "BRONZE_TABLE_PATH = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_bronze_data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ee8666-c857-4324-a2ae-05a3ff98c03d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. Get Tickers from Databricks SQL Table ---\n",
    "try:\n",
    "    sql_query = f\"SELECT Ticket, company_name FROM {sql_table}\"\n",
    "    spark_df_tickets = spark.sql(sql_query)\n",
    "    df_tickets_list = spark_df_tickets.collect()\n",
    "    company_info_dict = {row['Ticket']: row['company_name'] for row in df_tickets_list}\n",
    "    gpw_ticket_list = list(company_info_dict.keys())\n",
    "    print(\"Successfully created Spark DataFrame from SQL table.\")\n",
    "    print(\"Tickers to process:\", gpw_ticket_list)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from SQL table: {e}.\")\n",
    "    gpw_ticket_list = []\n",
    "    company_info_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f0b22f-3005-49f4-864a-c287022f0f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the base directory if it doesn't exist\n",
    "try:\n",
    "    dbutils.fs.mkdirs(BRONZE_TABLE_PATH)\n",
    "    print(f\"Successfully created directory for the SINGLE BRONZE TABLE: {BRONZE_TABLE_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Directory already exists or an error occurred during creation: {e}\")\n",
    "\n",
    "# Set end date for yfinance download\n",
    "end_date = date.today()\n",
    "\n",
    "if not gpw_ticket_list:\n",
    "    print(\"No company data to process. Using fallback data for demonstration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac5549e9-4cb4-4522-9727-2584c21852e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to check if a path is a Delta Table\n",
    "def is_delta_table(path):\n",
    "    \"\"\"Checks if the given path contains a _delta_log folder (i.e., is a Delta Table).\"\"\"\n",
    "    try:\n",
    "        # dbutils.fs.ls will raise an error if the path does not exist.\n",
    "        dbutils.fs.ls(os.path.join(path, \"_delta_log\")) \n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "#schema\n",
    "bronze_schema = T.StructType([\n",
    "    T.StructField(\"Date\", T.DateType(), True),\n",
    "    T.StructField(\"Open\", T.DoubleType(), True),\n",
    "    T.StructField(\"High\", T.DoubleType(), True),\n",
    "    T.StructField(\"Low\", T.DoubleType(), True),\n",
    "    T.StructField(\"Close\", T.DoubleType(), True),\n",
    "    T.StructField(\"Volume\", T.LongType(), True)\n",
    "])\n",
    "\n",
    "try:\n",
    "    print(f\"Starting Bronze Ingestion for environment: {ENV}\")\n",
    "    log_execution(spark, \"01_BRONZE_INGESTION\", \"STARTED\", LOGS_PATH)\n",
    "\n",
    "    is_full_table_existing = is_delta_table(BRONZE_TABLE_PATH)\n",
    "\n",
    "    print(\"\\nStarting data download from yfinance and saving to Delta Lake...\")\n",
    "\n",
    "    # --- 4. Main Ingestion Loop (Per Ticker) ---\n",
    "    for t in gpw_ticket_list:\n",
    "        yfinance_ticker = f\"{t}.WA\" # Assumes tickers are for Warsaw Stock Exchange (.WA)\n",
    "        print(f\"\\nProcessing stock: {yfinance_ticker}\")\n",
    "\n",
    "        # --- DOWNLOAD LOGIC ---\n",
    "        start_date_for_update = None\n",
    "        last_download_date = None\n",
    "\n",
    "        if is_full_table_existing:\n",
    "            try:\n",
    "                # READ FROM SINGLE LARGE TABLE: Read the maximum date FOR THIS SPECIFIC TICKER\n",
    "                # Using a filter predicate that acts as partition pruning\n",
    "                max_date_df = spark.read.format(\"delta\").load(BRONZE_TABLE_PATH) \\\n",
    "                                    .filter(F.col(\"Ticket\") == t) \\\n",
    "                                    .select(F.max(F.col(\"Date\").cast(\"date\")).alias(\"max_date\"))\n",
    "\n",
    "                last_download_date = max_date_df.collect()[0]['max_date']\n",
    "\n",
    "                if last_download_date and last_download_date < date.today():\n",
    "                    # Set start date to the day after the last downloaded date\n",
    "                    start_date_for_update = (pd.to_datetime(last_download_date) + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "                    print(f\"Data for '{yfinance_ticker}' requires update starting from date: {start_date_for_update}.\")\n",
    "                elif last_download_date:\n",
    "                    print(f\"Data for '{yfinance_ticker}' is up to date as of {last_download_date}. Skipping.\")\n",
    "                    time.sleep(1) # Avoid overwhelming the API\n",
    "                    continue\n",
    "                else:\n",
    "                    # If last_download_date is None (ticker not in the table or table is empty)\n",
    "                    print(f\"Ticker '{yfinance_ticker}' not found in the main table. Full download initiated.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while reading/filtering the main Delta Table for '{yfinance_ticker}': {e}. Initiating full re-download for safety.\")\n",
    "                # If there's an error, we rely on the logic below to handle full download\n",
    "                \n",
    "        # Set the starting date for yfinance\n",
    "        data = None\n",
    "        if not is_full_table_existing or not last_download_date:\n",
    "            # Full download from max available period (if table didn't exist or ticker was new)\n",
    "            data = yf.download(yfinance_ticker, period='max', end=end_date)\n",
    "            print(f\"Starting full download for '{yfinance_ticker}'.\")\n",
    "        elif start_date_for_update:\n",
    "            # Incremental download (if table existed and ticker needed updating)\n",
    "            data = yf.download(yfinance_ticker, start=start_date_for_update, end=end_date)\n",
    "            print(f\"Downloading incremental data for '{yfinance_ticker}'.\")\n",
    "        else:\n",
    "            # Data is up to date (handled by 'continue' in the block above, but safe guard)\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "\n",
    "        # --- 5. Delta Lake Write/Merge Logic ---\n",
    "        if data is not None and not data.empty:\n",
    "            # Prepare data for Spark DataFrame write\n",
    "            df_new_data = data.reset_index()\n",
    "            \n",
    "            # Handle MultiIndex after download (if it occurs)\n",
    "            if isinstance(df_new_data.columns, pd.MultiIndex):\n",
    "                df_new_data.columns = df_new_data.columns.map(lambda x: x[0])\n",
    "                \n",
    "            # Convert to Spark DataFrame\n",
    "            spark_new_data_df = spark.createDataFrame(df_new_data, schema=bronze_schema)\n",
    "\n",
    "            # Cleanup and Add Columns\n",
    "            spark_new_data_df = spark_new_data_df.withColumn(\"Date\", col(\"Date\").cast(DateType())) \\\n",
    "                                                .withColumn(\"Ticket\", lit(t)) \\\n",
    "                                                .withColumn(\"company_name\", lit(company_info_dict.get(t, 'N/A')))\n",
    "                                                \n",
    "            # Ensure DF has the required columns\n",
    "            cols_to_select = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Ticket', 'company_name']\n",
    "            spark_new_data_df = spark_new_data_df.select([F.col(c) for c in cols_to_select if c in spark_new_data_df.columns])\n",
    "\n",
    "            # A) If table DOES NOT EXIST (first run), create a new Delta Lake table\n",
    "            if not is_full_table_existing:\n",
    "                try:\n",
    "                    # Create the first version of the SINGLE MAIN Delta Table (partitioned by Ticket)\n",
    "                    spark_new_data_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"Ticket\").save(BRONZE_TABLE_PATH)\n",
    "                    print(f\"Downloaded and saved **first full batch** to the SINGLE Delta Table.\")\n",
    "                    #constraints\n",
    "                    spark.sql(f\"ALTER TABLE delta.`{BRONZE_TABLE_PATH}` ADD CONSTRAINT date_not_null CHECK (Date IS NOT NULL)\")\n",
    "                    spark.sql(f\"ALTER TABLE delta.`{BRONZE_TABLE_PATH}` ADD CONSTRAINT ticket_not_null CHECK (Ticket IS NOT NULL)\")\n",
    "                    is_full_table_existing = True # Mark that the table now exists\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR WRITING FULL DELTA TABLE for '{yfinance_ticker}': {e}\")\n",
    "                    \n",
    "            # B) If the table EXISTS, use MERGE\n",
    "            else:\n",
    "                try:\n",
    "                    # Use MERGE INTO for transactional update/insert in the SINGLE MAIN TABLE\n",
    "                    deltaTable = DeltaTable.forPath(spark, BRONZE_TABLE_PATH)\n",
    "                    \n",
    "                    deltaTable.alias(\"target\") \\\n",
    "                        .merge(\n",
    "                            source=spark_new_data_df.alias(\"source\"),\n",
    "                            # Join condition based on 'Date' and 'Ticket' (business key columns)\n",
    "                            condition=\"target.Date = source.Date AND target.Ticket = source.Ticket\"\n",
    "                        ) \\\n",
    "                        .whenMatchedUpdateAll() \\\n",
    "                        .whenNotMatchedInsertAll() \\\n",
    "                        .execute()\n",
    "                    \n",
    "                    print(f\"Updated data for '{yfinance_ticker}' using **MERGE INTO** in the SINGLE Delta Lake Table.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR MERGE INTO DELTA LAKE for '{yfinance_ticker}': {e}.\")\n",
    "                    \n",
    "        else:\n",
    "            print(f\"No new data to download or process for '{yfinance_ticker}'.\")\n",
    "            \n",
    "        time.sleep(0.5) # Avoid overwhelming the API\n",
    "    log_execution(spark, \"01_BRONZE_INGESTION\", \"SUCCESS\", LOGS_PATH)    \n",
    "    print(\"\\n--- COMPLETED UPDATE/INSERT PROCESS TO THE SINGLE MAIN DELTA LAKE TABLE ---\")\n",
    "except Exception as e:\n",
    "    # --- BŁĄD ---\n",
    "    error_msg = str(e)[:500]\n",
    "    log_execution(spark, \"01_BRONZE_INGESTION\", \"FAILED\", LOGS_PATH, message=error_msg)\n",
    "    print(f\"\\nFATAL ERROR in Bronze process: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666ecc64-6c91-45ce-9a3e-30c201e845b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Yfinance download",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
