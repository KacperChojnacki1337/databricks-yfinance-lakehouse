{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec1190c0-d9ce-459c-8c23-6297afb9bc44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ../../requirements.txt\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0eb81c4-436c-40f0-b971-3c1db1e2e6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, TimestampType\n",
    "import yaml\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql import types as T\n",
    "# --- DODATKOWY IMPORT DLA DELTA LAKE ---\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "try:\n",
    "    # dbutils.widgets.get() jest standardową metodą odczytu parametrów Joba\n",
    "    ENV = dbutils.widgets.get(\"env_name\") \n",
    "    print(f\"Środowisko skonfigurowane przez Job (ENV): {ENV}\")\n",
    "except Exception:\n",
    "    # Obsługa błędu - gdy uruchamiasz notebook interaktywnie lub ręcznie (spoza Joba),\n",
    "    # dbutils.widgets.get() może zgłosić błąd, bo widgety nie są zainicjowane.\n",
    "    # W takim przypadku ustawiamy wartość domyślną (np. 'TEST')\n",
    "    ENV = 'TEST' \n",
    "    print(f\"Uruchomienie interaktywne/ręczne. Użycie domyślnego ENV: {ENV}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # W Databricks, ścieżka do pliku YAML może wymagać dostosowania,\n",
    "    # jeśli nie jest on w tym samym katalogu co notatnik.\n",
    "    with open('../../config/config.yaml', 'r') as file:\n",
    "        full_config = yaml.safe_load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"BŁĄD: Plik 'config.yaml' nie został znaleziony! Sprawdź ścieżkę.\")\n",
    "    raise\n",
    "\n",
    "CFG = full_config.get(ENV)\n",
    "if not CFG:\n",
    "    raise ValueError(f\"Nie znaleziono konfiguracji dla środowiska: {ENV} w pliku YAML.\")\n",
    "\n",
    "catalog_name = CFG['catalog_name']\n",
    "schema_name = CFG['schema_name']\n",
    "volume_name = CFG['volume_name']\n",
    "sql_table = CFG['sql_table_name']\n",
    "BRONZE_TABLE_PATH = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_bronze_data/\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ee8666-c857-4324-a2ae-05a3ff98c03d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- KROK 1: Tworzenie df_tickets z tabeli Databricks SQL ---\n",
    "try:\n",
    "    sql_query = f\"SELECT Ticket, company_name FROM {sql_table}\"\n",
    "    spark_df_tickets = spark.sql(sql_query)\n",
    "    df_tickets_list = spark_df_tickets.collect()\n",
    "    company_info_dict = {row['Ticket']: row['company_name'] for row in df_tickets_list}\n",
    "    gpw_ticket_list = list(company_info_dict.keys())\n",
    "    print(\"Pomyślnie utworzono Spark DataFrame z tabeli SQL.\")\n",
    "    print(\"Akcje do przetworzenia:\", gpw_ticket_list)\n",
    "except Exception as e:\n",
    "    print(f\"Błąd odczytu z tabeli SQL: {e}.\")\n",
    "    gpw_ticket_list = []\n",
    "    company_info_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f0b22f-3005-49f4-864a-c287022f0f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Utworzenie katalogu bazowego, jeśli nie istnieje\n",
    "try:\n",
    "    dbutils.fs.mkdirs(BRONZE_TABLE_PATH)\n",
    "    print(f\"Pomyślnie utworzono katalog dla JEDNEJ TABELI: {BRONZE_TABLE_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Katalog już istnieje lub wystąpił błąd podczas jego tworzenia: {e}\")\n",
    "\n",
    "# Ustawienia zakresu dat dla yfinance\n",
    "end_date = date.today()\n",
    "\n",
    "if not gpw_ticket_list:\n",
    "    print(\"Brak danych firm do przetworzenia. Używanie danych zastępczych dla demonstracji.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac5549e9-4cb4-4522-9727-2584c21852e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to check if a path is a Delta Table\n",
    "def is_delta_table(path):\n",
    "    \"\"\"Sprawdza, czy podana ścieżka zawiera folder _delta_log (czyli jest Delta Table).\"\"\"\n",
    "    try:\n",
    "        # dbutils.fs.ls zgłosi błąd, jeśli ścieżka nie istnieje.\n",
    "        dbutils.fs.ls(os.path.join(path, \"_delta_log\")) \n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "is_full_table_existing = is_delta_table(BRONZE_TABLE_PATH)\n",
    "\n",
    "print(\"\\nRozpoczynanie pobierania danych z yfinance i zapisywanie do Delta Lake...\")\n",
    "\n",
    "for t in gpw_ticket_list:\n",
    "    yfinance_ticker = f\"{t}.WA\"\n",
    "    print(f\"Przetwarzanie akcji: {yfinance_ticker}\")\n",
    "\n",
    "    # --- LOGIKA POBIERANIA DANYCH ---\n",
    "    start_date_for_update = None\n",
    "    last_download_date = None\n",
    "\n",
    "    if is_full_table_existing:\n",
    "        try:\n",
    "            # ODZCYT Z JEDNEJ DUŻEJ TABELI: Odczytujemy maksymalną datę DLA TEGO KONKRETNEGO TICKERA\n",
    "            # Używamy predykatu filtrującego, który zadziała jako pruning na partycjach\n",
    "            max_date_df = spark.read.format(\"delta\").load(BRONZE_TABLE_PATH) \\\n",
    "                            .filter(F.col(\"Ticket\") == t) \\\n",
    "                            .select(F.max(F.col(\"Date\").cast(\"date\")).alias(\"max_date\"))\n",
    "\n",
    "            last_download_date = max_date_df.collect()[0]['max_date']\n",
    "\n",
    "            if last_download_date and last_download_date < date.today():\n",
    "                # Ustawiamy datę początkową na dzień po ostatnio pobranym\n",
    "                start_date_for_update = (pd.to_datetime(last_download_date) + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "                print(f\"Dane dla '{yfinance_ticker}' wymagają aktualizacji od daty: {start_date_for_update}.\")\n",
    "            elif last_download_date:\n",
    "                print(f\"Dane dla '{yfinance_ticker}' są aktualne na dzień {last_download_date}. Pomijanie.\")\n",
    "                time.sleep(1) # Unikaj przeciążania API\n",
    "                continue\n",
    "            else:\n",
    "                # Jeśli last_download_date jest None (ticker nie istnieje w tabeli lub tabela pusta)\n",
    "                print(f\"Ticker '{yfinance_ticker}' nie został znaleziony w głównej tabeli. Pełne pobranie.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Wystąpił błąd odczytu/filtra na głównej Delta Table dla '{yfinance_ticker}': {e}. Pełne ponowne pobranie.\")\n",
    "            # Jeśli jest błąd, pozostawiamy is_full_table_existing na True, \n",
    "            # ale proceedujemy do pełnego pobierania dla bezpieczeństwa.\n",
    "    \n",
    "            \n",
    "    # Ustawienia daty początkowej dla yfinance\n",
    "    if not is_full_table_existing or not last_download_date:\n",
    "        # Pełne pobieranie od maksymalnej dostępnej daty (gdy tabela nie istniala lub ticker nie istniał)\n",
    "        data = yf.download(yfinance_ticker, period='max', end=end_date)\n",
    "        print(f\"Rozpoczęcie pełnego pobierania dla '{yfinance_ticker}'.\")\n",
    "    elif start_date_for_update:\n",
    "        # Pobieranie tylko nowych danych (gdy tabela istniala i ticker istnial)\n",
    "        data = yf.download(yfinance_ticker, start=start_date_for_update, end=end_date)\n",
    "        print(f\"Pobieranie danych przyrostowych dla '{yfinance_ticker}'.\")\n",
    "    else:\n",
    "        # Dane aktualne (obsłużone przez 'continue' w bloku powyżej, ale zabezpieczenie)\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "\n",
    "    # --- LOGIKA ZAPISU DO DELTA LAKE ---\n",
    "    if not data.empty:\n",
    "        # Przygotowanie danych do zapisu Spark DataFrame\n",
    "        df_new_data = data.reset_index()\n",
    "        # Obsługa MultiIndex po pobraniu (jeśli wystąpi)\n",
    "        if isinstance(df_new_data.columns, pd.MultiIndex):\n",
    "            df_new_data.columns = df_new_data.columns.map(lambda x: x[0])\n",
    "        # Konwersja do Spark DataFrame\n",
    "        spark_new_data_df = spark.createDataFrame(df_new_data)\n",
    "\n",
    "        # Czyszczenie i dodawanie kolumn\n",
    "        spark_new_data_df = spark_new_data_df.withColumn(\"Date\", col(\"Date\").cast(\"date\")) \\\n",
    "                                              .withColumn(\"Ticket\", lit(t)) \\\n",
    "                                              .withColumn(\"company_name\", lit(company_info_dict.get(t, 'N/A')))\n",
    "        # Upewnienie się, że DF ma wymagane kolumny\n",
    "        cols_to_select = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Ticket', 'company_name']\n",
    "        spark_new_data_df = spark_new_data_df.select([F.col(c) for c in cols_to_select if c in spark_new_data_df.columns])\n",
    "\n",
    "        # A) Jeśli tabela NIE ISTNIEJE (po pierwszym przebiegu), tworzymy nową jako Delta Lake\n",
    "        if not is_full_table_existing:\n",
    "            try:\n",
    "                # Tworzymy pierwszą wersję JEDNEJ GŁÓWNEJ TABELI Delta (partycjonowanej przez Ticket)\n",
    "                spark_new_data_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"Ticket\").save(BRONZE_TABLE_PATH)\n",
    "                print(f\"Pobrano i zapisano **pierwszą pełną** partię do JEDNEJ TABELI Delta.\")\n",
    "                is_full_table_existing = True # Oznaczamy, że tabela już istnieje\n",
    "            except Exception as e:\n",
    "                print(f\"BŁĄD ZAPISU PEŁNEJ DELTA TABLE dla '{yfinance_ticker}': {e}\")\n",
    "        # B) Jeśli tabela ISTNIEJE, używamy MERGE\n",
    "        else:\n",
    "            try:\n",
    "                # Używamy MERGE INTO do transakcyjnej aktualizacji/wstawiania w JEDNEJ GŁÓWNEJ TABELI\n",
    "                deltaTable = DeltaTable.forPath(spark, BRONZE_TABLE_PATH)\n",
    "                \n",
    "                deltaTable.alias(\"target\") \\\n",
    "                    .merge(\n",
    "                        source=spark_new_data_df.alias(\"source\"),\n",
    "                        # Łączymy na podstawie 'Date' i 'Ticket' (kolumny klucza biznesowego)\n",
    "                        condition=\"target.Date = source.Date AND target.Ticket = source.Ticket\"\n",
    "                    ) \\\n",
    "                    .whenMatchedUpdateAll() \\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .execute()\n",
    "                \n",
    "                print(f\"Zaktualizowano dane dla '{yfinance_ticker}' za pomocą **MERGE INTO** w JEDNEJ TABELI Delta Lake.\")\n",
    "            except Exception as e:\n",
    "                print(f\"BŁĄD MERGE INTO DELTA LAKE dla '{yfinance_ticker}': {e}.\")\n",
    "                \n",
    "    else:\n",
    "        print(f\"Brak nowych danych do pobrania dla '{yfinance_ticker}'.\")\n",
    "        \n",
    "    time.sleep(1) # Unikaj przeciążania API\n",
    "    \n",
    "print(\"\\n--- ZAKOŃCZONO PROCES AKTUALIZACJI/WSTAWIANIA DO JEDNEJ GŁÓWNEJ TABELI DELTA LAKE ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666ecc64-6c91-45ce-9a3e-30c201e845b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Yfinance download",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
