{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1711359a-4a11-4ae4-904d-92f197216f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.types import IntegerType, FloatType, DateType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from libs.logger import log_execution\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "\n",
    "# --- 1. Environment Configuration ---\n",
    "try:\n",
    "    # Get environment variable from Databricks widget or set default\n",
    "    ENV = dbutils.widgets.get(\"env_name\")\n",
    "except Exception:\n",
    "    ENV = 'TEST'\n",
    "    \n",
    "# --- 2. Load Configuration ---\n",
    "try:\n",
    "    # Load configuration from the YAML file\n",
    "    with open('../../config/config.yaml', 'r') as file:\n",
    "        full_config = yaml.safe_load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'config.yaml' file not found! Check the path.\")\n",
    "    raise\n",
    "\n",
    "CFG = full_config.get(ENV)\n",
    "if not CFG:\n",
    "    raise ValueError(f\"Configuration not found for environment: {ENV} in YAML file.\")\n",
    "\n",
    "catalog_name = CFG['catalog_name']\n",
    "schema_name = CFG['schema_name']\n",
    "volume_name = CFG['volume_name']\n",
    "\n",
    "# Paths\n",
    "LOGS_PATH = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/silver_execution_logs/\"\n",
    "base_bronze_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_bronze_data\"\n",
    "silver_table_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_silver_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e8704c6-e260-4b8a-88a3-aa951275550f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#schema from bronze\n",
    "bronze_input_schema = T.StructType([\n",
    "    T.StructField(\"Date\", T.DateType(), True),\n",
    "    T.StructField(\"Open\", T.DoubleType(), True),\n",
    "    T.StructField(\"High\", T.DoubleType(), True),\n",
    "    T.StructField(\"Low\", T.DoubleType(), True),\n",
    "    T.StructField(\"Close\", T.DoubleType(), True),\n",
    "    T.StructField(\"Volume\", T.LongType(), True),\n",
    "    T.StructField(\"Ticket\", T.StringType(), True),\n",
    "    T.StructField(\"company_name\", T.StringType(), True)\n",
    "])\n",
    "try:\n",
    "    print(f\"--- STARTING SILVER LAYER PROCESS (Mode: Full Overwrite) ---\")\n",
    "    log_execution(spark, \"02_SILVER_TRANSFORMATION\", \"STARTED\", LOGS_PATH)\n",
    "\n",
    "    # --- 3. BRONZE LAYER: Read & Type Optimization ---\n",
    "    df_bronze = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .schema(bronze_input_schema) \n",
    "        .load(base_bronze_path)\n",
    "    )\n",
    "\n",
    "    df_optimized = (\n",
    "        df_bronze\n",
    "        .withColumn(\"Date\", col(\"Date\").cast(DateType()))\n",
    "        .withColumn(\"Open\", col(\"Open\").cast(FloatType()))\n",
    "        .withColumn(\"High\", col(\"High\").cast(FloatType()))\n",
    "        .withColumn(\"Low\", col(\"Low\").cast(FloatType()))\n",
    "        .withColumn(\"Close\", col(\"Close\").cast(FloatType()))\n",
    "        .withColumn(\"Volume\", col(\"Volume\").cast(IntegerType())) \n",
    "    )\n",
    "\n",
    "    # --- 4. DATA QUALITY CHECKS ---\n",
    "    \n",
    "    # 1. Critical Null Check\n",
    "    critical_null_count = df_optimized.filter(\n",
    "        F.col(\"Date\").isNull() | F.col(\"Ticket\").isNull() | F.col(\"Close\").isNull()\n",
    "    ).count()\n",
    "\n",
    "    if critical_null_count > 0:\n",
    "        raise ValueError(f\"QA ERROR: Found {critical_null_count} rows with critical NULLs. Pipeline halted.\")\n",
    "\n",
    "    # 2. Duplicate Check & Removal\n",
    "    total_rows = df_optimized.count()\n",
    "    df_unique = df_optimized.dropDuplicates(subset=[\"Date\", \"Ticket\"])\n",
    "    unique_rows = df_unique.count()\n",
    "\n",
    "    if total_rows != unique_rows:\n",
    "        print(f\"WARNING: Removed {total_rows - unique_rows} duplicates.\")\n",
    "\n",
    "    # --- 5. SILVER LAYER: Feature Engineering ---\n",
    "    PRECISION = 4\n",
    "    SMA_PERIODS = [20, 50, 200]\n",
    "    window_spec = Window.partitionBy(\"Ticket\").orderBy(\"Date\")\n",
    "\n",
    "    # A) Daily Returns\n",
    "    df_silver = df_unique.withColumn(\n",
    "        \"Previous_Close\", F.lag(F.col(\"Close\"), 1).over(window_spec)\n",
    "    ).withColumn(\n",
    "        \"Daily_Return_Pct\", \n",
    "        F.round(((F.col(\"Close\") - F.col(\"Previous_Close\")) / F.col(\"Previous_Close\")) * 100, PRECISION).cast(FloatType())\n",
    "    ).drop(\"Previous_Close\")\n",
    "\n",
    "    # B) Time Dimensions\n",
    "    df_silver = (\n",
    "        df_silver\n",
    "        .withColumn(\"Year\", F.year(F.col(\"Date\")))\n",
    "        .withColumn(\"Quarter\", F.quarter(F.col(\"Date\")))\n",
    "        .withColumn(\"Month\", F.month(F.col(\"Date\")))\n",
    "        .withColumn(\"WeekOfYear\", F.weekofyear(F.col(\"Date\")))\n",
    "    )\n",
    "\n",
    "    # C) Simple Moving Averages (SMA)\n",
    "    for N in SMA_PERIODS:\n",
    "        window_sma = window_spec.rowsBetween(-(N - 1), 0)\n",
    "        df_silver = df_silver.withColumn(\n",
    "            f\"SMA_{N}\",\n",
    "            F.avg(F.col(\"Close\")).over(window_sma).cast(FloatType())\n",
    "        )\n",
    "\n",
    "    # --- 6. WRITE TO DELTA LAKE (Full Overwrite) ---\n",
    "    (\n",
    "        df_silver.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"Ticket\") # Optymalizacja pod odczyt konkretnych spółek\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .save(silver_table_path)\n",
    "    )\n",
    "\n",
    "    # --- 7. POST-WRITE OPTIMIZATION & CONSTRAINTS ---\n",
    "    \n",
    "\n",
    "    spark.sql(f\"OPTIMIZE delta.`{silver_table_path}` ZORDER BY (Date)\")\n",
    "    spark.sql(f\"VACUUM delta.`{silver_table_path}` RETAIN 168 HOURS\")\n",
    "    print(f\"Successfully saved data to Silver layer at: {silver_table_path}\")\n",
    "    log_execution(spark, \"02_SILVER_TRANSFORMATION\", \"SUCCESS\", LOGS_PATH)\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = str(e)[:500]\n",
    "    log_execution(spark, \"02_SILVER_TRANSFORMATION\", \"FAILED\", LOGS_PATH, message=error_msg)\n",
    "    print(f\"FATAL ERROR in Silver process: {e}\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Yfinance-silver-layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
