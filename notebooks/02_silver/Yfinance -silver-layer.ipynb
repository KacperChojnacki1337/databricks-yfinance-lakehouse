{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1711359a-4a11-4ae4-904d-92f197216f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "# 1. Konfiguracja Środowiska\n",
    "try:\n",
    "    ENV = dbutils.widgets.get(\"env_name\")\n",
    "except Exception:\n",
    "    ENV = 'TEST'\n",
    "    \n",
    "# 2. Wczytanie Konfiguracji\n",
    "try:\n",
    "    with open('../../config/config.yaml', 'r') as file:\n",
    "        full_config = yaml.safe_load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"BŁĄD: Plik 'config.yaml' nie został znaleziony! Sprawdź ścieżkę.\")\n",
    "    raise\n",
    "\n",
    "CFG = full_config.get(ENV)\n",
    "if not CFG:\n",
    "    raise ValueError(f\"Nie znaleziono konfiguracji dla środowiska: {ENV} w pliku YAML.\")\n",
    "\n",
    "catalog_name = CFG['catalog_name']\n",
    "schema_name = CFG['schema_name']\n",
    "volume_name = CFG['volume_name']\n",
    "\n",
    "base_output_directory = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_bronze_data\"\n",
    "print(f\"Ścieżka do tabeli Delta Bronze: {base_output_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e8704c6-e260-4b8a-88a3-aa951275550f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#wczytywanie danych z tabeli delta\n",
    "try:\n",
    "    # Używamy formatu \"delta\" do odczytu folderu Delta Lake\n",
    "    df_bronze = (\n",
    "        spark.read.format(\"delta\") \n",
    "        .load(base_output_directory) # Ładowanie ze ścieżki do folderu Delta\n",
    "        .withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "    )\n",
    "    \n",
    "    print(f\"Pomyślnie wczytano dane. Liczba wierszy: {df_bronze.count()}\")\n",
    "   \n",
    "except Exception as e:\n",
    "    print(f\"BŁĄD: Wystąpił problem podczas ładowania danych: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed2fe81-7714-453c-948c-4e91a742e40d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 2. Zastosowanie optymalizacji typów danych\n",
    "df_optimized = (\n",
    "    df_bronze\n",
    "    .withColumn(\"Date\", col(\"Date\").cast(\"date\")) # Data: pozostaje 'date'\n",
    "    \n",
    "    # Zmiana z 'double' na 'float' (lub 'FloatType()')\n",
    "    .withColumn(\"Open\", col(\"Open\").cast(FloatType()))\n",
    "    .withColumn(\"High\", col(\"High\").cast(FloatType()))\n",
    "    .withColumn(\"Low\", col(\"Low\").cast(FloatType()))\n",
    "    .withColumn(\"Close\", col(\"Close\").cast(FloatType()))\n",
    "    \n",
    "    # Zmiana z 'long' na 'integer' (lub 'IntegerType()')\n",
    "    .withColumn(\"Volume\", col(\"Volume\").cast(IntegerType())) \n",
    "    # 'Ticket' i 'company_name' pozostają 'string'\n",
    ")\n",
    "\n",
    "# Podmiana oryginalnej zmiennej na zoptymalizowany DataFrame\n",
    "df_bronze = df_optimized\n",
    "\n",
    "print(f\"Zoptymalizowano schemat.\")\n",
    "df_bronze.printSchema()\n",
    "\n",
    "df_bronze.show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c12bf5d2-fd35-4fe8-9c21-4f39c0c29581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Minimalna Walidacja Krytycznych Kolumn Pod Kątem nulli\n",
    "critical_null_count = df_bronze.filter(\n",
    "    F.col(\"Date\").isNull() | \n",
    "    F.col(\"Ticket\").isNull() | \n",
    "    F.col(\"Close\").isNull()\n",
    ").count()\n",
    "\n",
    "if critical_null_count > 0:\n",
    "    # Wyrzuć błąd, który zatrzyma potok ETL i wymaga interwencji\n",
    "    raise ValueError(f\"BŁĄD QA: Znaleziono {critical_null_count} wierszy z NULLami w kolumnach krytycznych (Date, Ticket, Close). Potok zatrzymany.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "696cf23c-c70e-403b-8ec5-2c0fc51c88c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sprawdzanie duplikatów po Date&Ticket\n",
    "deduplication_key = [\"Date\", \"Ticket\"]\n",
    "total_rows = df_bronze.count()\n",
    "df_unique = df_bronze.dropDuplicates(subset=deduplication_key)\n",
    "unique_rows = df_unique.count()\n",
    "duplicate_count = total_rows - unique_rows\n",
    "duplicate_percentage = (duplicate_count / total_rows) * 100\n",
    "\n",
    "print(f\"--- Raport Duplikatów ---\")\n",
    "print(f\"Całkowita liczba wierszy: {total_rows}\")\n",
    "print(f\"Liczba unikalnych wierszy: {unique_rows}\")\n",
    "print(f\"Liczba znalezionych duplikatów: {duplicate_count}\")\n",
    "print(f\"Procent duplikatów: {duplicate_percentage}%\")\n",
    "\n",
    "# IMPLEMENTACJA ZATRZYMANIA KODU\n",
    "if total_rows != unique_rows:\n",
    "    # Używamy ValueError, aby zatrzymać potok i poinformować o błędzie\n",
    "    raise ValueError(\n",
    "        f\"BŁĄD QA: Znaleziono duplikaty w kluczu (Date, Ticket)! \"\n",
    "        f\"Łącznie: {duplicate_count} wierszy ({duplicate_percentage}%). \"\n",
    "        f\"Potok ETL został zatrzymany w celu inspekcji danych źródłowych.\"\n",
    "    )\n",
    "    \n",
    "# Jeśli duplikatów nie ma, kontynuujemy i nadpisujemy (lub po prostu używamy) df_unique\n",
    "df_bronze = df_unique \n",
    "print(\"WALIDACJA DUPLIKATÓW: ZAKOŃCZONA POWODZENIEM. Przechodzę dalej.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Yfinance -silver-layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
