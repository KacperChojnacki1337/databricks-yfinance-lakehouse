{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1711359a-4a11-4ae4-904d-92f197216f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 1. Konfiguracja Środowiska\n",
    "try:\n",
    "    ENV = dbutils.widgets.get(\"env_name\")\n",
    "except Exception:\n",
    "    ENV = 'TEST'\n",
    "    \n",
    "# 2. Wczytanie Konfiguracji\n",
    "try:\n",
    "    with open('../../config/config.yaml', 'r') as file:\n",
    "        full_config = yaml.safe_load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"BŁĄD: Plik 'config.yaml' nie został znaleziony! Sprawdź ścieżkę.\")\n",
    "    raise\n",
    "\n",
    "CFG = full_config.get(ENV)\n",
    "if not CFG:\n",
    "    raise ValueError(f\"Nie znaleziono konfiguracji dla środowiska: {ENV} w pliku YAML.\")\n",
    "\n",
    "catalog_name = CFG['catalog_name']\n",
    "schema_name = CFG['schema_name']\n",
    "volume_name = CFG['volume_name']\n",
    "\n",
    "base_output_directory = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_bronze_data\"\n",
    "print(f\"Ścieżka do tabeli Delta Bronze: {base_output_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e8704c6-e260-4b8a-88a3-aa951275550f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#wczytywanie danych z tabeli delta\n",
    "try:\n",
    "    # Używamy formatu \"delta\" do odczytu folderu Delta Lake\n",
    "    df_bronze = (\n",
    "        spark.read.format(\"delta\") \n",
    "        .load(base_output_directory) # Ładowanie ze ścieżki do folderu Delta\n",
    "        .withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "    )\n",
    "    \n",
    "    print(f\"Pomyślnie wczytano dane. Liczba wierszy: {df_bronze.count()}\")\n",
    "   \n",
    "except Exception as e:\n",
    "    print(f\"BŁĄD: Wystąpił problem podczas ładowania danych: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed2fe81-7714-453c-948c-4e91a742e40d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 2. Zastosowanie optymalizacji typów danych\n",
    "df_optimized = (\n",
    "    df_bronze\n",
    "    .withColumn(\"Date\", col(\"Date\").cast(\"date\")) # Data: pozostaje 'date'\n",
    "    \n",
    "    # Zmiana z 'double' na 'float' (lub 'FloatType()')\n",
    "    .withColumn(\"Open\", col(\"Open\").cast(FloatType()))\n",
    "    .withColumn(\"High\", col(\"High\").cast(FloatType()))\n",
    "    .withColumn(\"Low\", col(\"Low\").cast(FloatType()))\n",
    "    .withColumn(\"Close\", col(\"Close\").cast(FloatType()))\n",
    "    \n",
    "    # Zmiana z 'long' na 'integer' (lub 'IntegerType()')\n",
    "    .withColumn(\"Volume\", col(\"Volume\").cast(IntegerType())) \n",
    "    # 'Ticket' i 'company_name' pozostają 'string'\n",
    ")\n",
    "\n",
    "# Podmiana oryginalnej zmiennej na zoptymalizowany DataFrame\n",
    "df_bronze = df_optimized\n",
    "\n",
    "print(f\"Zoptymalizowano schemat.\")\n",
    "df_bronze.printSchema()\n",
    "\n",
    "df_bronze.show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c12bf5d2-fd35-4fe8-9c21-4f39c0c29581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Minimalna Walidacja Krytycznych Kolumn Pod Kątem nulli\n",
    "critical_null_count = df_bronze.filter(\n",
    "    F.col(\"Date\").isNull() | \n",
    "    F.col(\"Ticket\").isNull() | \n",
    "    F.col(\"Close\").isNull()\n",
    ").count()\n",
    "\n",
    "if critical_null_count > 0:\n",
    "    # Wyrzuć błąd, który zatrzyma potok ETL i wymaga interwencji\n",
    "    raise ValueError(f\"BŁĄD QA: Znaleziono {critical_null_count} wierszy z NULLami w kolumnach krytycznych (Date, Ticket, Close). Potok zatrzymany.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "696cf23c-c70e-403b-8ec5-2c0fc51c88c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sprawdzanie duplikatów po Date&Ticket\n",
    "deduplication_key = [\"Date\", \"Ticket\"]\n",
    "total_rows = df_bronze.count()\n",
    "df_unique = df_bronze.dropDuplicates(subset=deduplication_key)\n",
    "unique_rows = df_unique.count()\n",
    "duplicate_count = total_rows - unique_rows\n",
    "duplicate_percentage = (duplicate_count / total_rows) * 100\n",
    "\n",
    "print(f\"--- Raport Duplikatów ---\")\n",
    "print(f\"Całkowita liczba wierszy: {total_rows}\")\n",
    "print(f\"Liczba unikalnych wierszy: {unique_rows}\")\n",
    "print(f\"Liczba znalezionych duplikatów: {duplicate_count}\")\n",
    "print(f\"Procent duplikatów: {duplicate_percentage}%\")\n",
    "\n",
    "# IMPLEMENTACJA ZATRZYMANIA KODU\n",
    "if total_rows != unique_rows:\n",
    "    # Używamy ValueError, aby zatrzymać potok i poinformować o błędzie\n",
    "    raise ValueError(\n",
    "        f\"BŁĄD QA: Znaleziono duplikaty w kluczu (Date, Ticket)! \"\n",
    "        f\"Łącznie: {duplicate_count} wierszy ({duplicate_percentage}%). \"\n",
    "        f\"Potok ETL został zatrzymany w celu inspekcji danych źródłowych.\"\n",
    "    )\n",
    "    \n",
    "# Jeśli duplikatów nie ma, kontynuujemy i nadpisujemy (lub po prostu używamy) df_unique\n",
    "df_bronze = df_unique \n",
    "print(\"WALIDACJA DUPLIKATÓW: ZAKOŃCZONA POWODZENIEM. Przechodzę dalej.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f1a603-d523-4642-b9a9-fcc05da7be29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType # Potrzebne do rzutowania\n",
    "\n",
    "# Zakładam, że df_bronze, F, Window, FloatType są zaimportowane i dostępne\n",
    "\n",
    "PRECISION = 4\n",
    "\n",
    "# Definicja Okna: Działamy w ramach każdego Tickera, sortując po dacie\n",
    "window_spec = (\n",
    "    Window.partitionBy(\"Ticket\")\n",
    "    .orderBy(\"Date\")\n",
    ")\n",
    "\n",
    "df_silver = (\n",
    "    df_bronze\n",
    "    .withColumn(\n",
    "        \"Previous_Close\", \n",
    "        # 1. Obliczenie ceny zamknięcia z poprzedniego dnia handlowego\n",
    "        F.lag(F.col(\"Close\"), 1).over(window_spec)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Daily_Return_Pct\", \n",
    "        # 2. Obliczenie procentowej zmiany i jawne zaokrąglenie do PRECISION miejsc\n",
    "        F.round(\n",
    "            ((F.col(\"Close\") - F.col(\"Previous_Close\")) / F.col(\"Previous_Close\")) * 100, \n",
    "            PRECISION # Użycie stałej 4\n",
    "        ).cast(FloatType())\n",
    "    )\n",
    "    .drop(\"Previous_Close\") # 3. Usunięcie tymczasowej kolumny\n",
    ")\n",
    "\n",
    "# Podmiana oryginalnej zmiennej na DataFrame z warstwy Silver\n",
    "df_silver\n",
    "\n",
    "print(f\"Pomyślnie obliczono i zaokrąglono 'Daily_Return_Pct' do {PRECISION} miejsc po przecinku.\")\n",
    "df_silver.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1107305-0a85-4cb8-b47f-928f85c5e666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = (\n",
    "    df_silver\n",
    "    .withColumn(\n",
    "        \"Year\", \n",
    "        F.year(F.col(\"Date\")) # Wyodrębnia rok (np. 2023)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Quarter\", \n",
    "        F.quarter(F.col(\"Date\")) # Wyodrębnia kwartał (1, 2, 3 lub 4)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Month\", \n",
    "        F.month(F.col(\"Date\")) # Wyodrębnia miesiąc (1 do 12)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"WeekOfYear\", \n",
    "        # Wyodrębnia numer tygodnia w roku (1 do 53)\n",
    "        F.weekofyear(F.col(\"Date\")) \n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Pomyślnie dodano kolumny Year, Quarter, Month, WeekOfYear.\")\n",
    "\n",
    "# Wyświetlenie nowych kolumn\n",
    "df_silver.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e77a3507-d18c-4651-a1ee-430ed6aceb99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Okresy SMA ---\n",
    "SMA_PERIODS = [20, 50, 200]\n",
    "base_window_spec = (\n",
    "    Window.partitionBy(\"Ticket\")\n",
    "    .orderBy(\"Date\")\n",
    ")\n",
    "\n",
    "# Inicjalizujemy DataFrame, na którym będziemy pracować (obecnie df_bronze zawiera już Daily_Return_Pct)\n",
    "df_silver_sma = df_silver\n",
    "\n",
    "# --- Iteracyjne tworzenie kolumn SMA ---\n",
    "for N in SMA_PERIODS:\n",
    "    # 1. Definicja Specyfikacji Okna dla danego okresu N\n",
    "    # rowsBetween(-(N-1), 0) oznacza: od N-1 wierszy wstecz do bieżącego wiersza (0)\n",
    "    window_spec_n = base_window_spec.rowsBetween(-(N - 1), 0)\n",
    "\n",
    "    column_name = f\"SMA_{N}\"\n",
    "    \n",
    "    # 2. Obliczenie Średniej Kroczącej (SMA)\n",
    "    df_silver_sma = df_silver_sma.withColumn(\n",
    "        column_name,\n",
    "        # Używamy funkcji avg() na kolumnie 'Close' w ramach zdefiniowanego okna\n",
    "        F.avg(F.col(\"Close\")).over(window_spec_n).cast(FloatType())\n",
    "    )\n",
    "df_silver = df_silver_sma\n",
    "\n",
    "print(f\"Pomyślnie dodano kolumny SMA dla okresów: {SMA_PERIODS}\")\n",
    "\n",
    "df_silver.show(100)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Yfinance -silver-layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
