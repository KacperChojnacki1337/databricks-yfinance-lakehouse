{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1711359a-4a11-4ae4-904d-92f197216f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Konfiguracja Środowiska (jak w notebooku Bronze)\n",
    "try:\n",
    "    # Zakładamy, że dbutils.widgets.get() jest dostępne w Databricks\n",
    "    ENV = dbutils.widgets.get(\"env_name\") \n",
    "except Exception:\n",
    "    ENV = 'TEST' \n",
    "    \n",
    "# 2. Wczytanie Konfiguracji\n",
    "try:\n",
    "    # Upewnij się, że ścieżka do config.yaml jest poprawna dla tego notebooka!\n",
    "    with open('../../config/config.yaml', 'r') as file:\n",
    "        full_config = yaml.safe_load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"BŁĄD: Plik 'config.yaml' nie został znaleziony! Sprawdź ścieżkę.\")\n",
    "    raise\n",
    "\n",
    "CFG = full_config.get(ENV)\n",
    "if not CFG:\n",
    "    raise ValueError(f\"Nie znaleziono konfiguracji dla środowiska: {ENV} w pliku YAML.\")\n",
    "\n",
    "catalog_name = CFG['catalog_name']\n",
    "schema_name = CFG['schema_name']\n",
    "volume_name = CFG['volume_name']\n",
    "\n",
    "# Użyj tej samej bazowej ścieżki wyjściowej, co w notebooku Bronze\n",
    "base_output_directory = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_delta_lake_bronze/\"\n",
    "print(f\"Ścieżka do warstwy Bronze: {base_output_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e8704c6-e260-4b8a-88a3-aa951275550f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ścieżka do katalogu nadrzędnego zawierającego podkatalogi Delta/Parquet\n",
    "base_output_directory = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_delta_lake_bronze/\"\n",
    "\n",
    "try:\n",
    "    # Kluczowa zmiana: uzywamy formatu \"parquet\" zamiast \"delta\"\n",
    "    df_bronze = (\n",
    "        spark.read.format(\"parquet\")\n",
    "        .load(base_output_directory)\n",
    "        .withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "    )\n",
    "    \n",
    "    print(f\"Pomyślnie wczytano dane. Liczba wierszy: {df_bronze.count()}\")\n",
    "    df_bronze.printSchema()\n",
    "    \n",
    "    # Zauważ, że kolumna 'Ticket' zostanie poprawnie wczytana jako kolumna partycji\n",
    "    df_bronze.select(\"Ticket\", \"Date\", \"Close\").show(5)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"BŁĄD: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Yfinance -silver-layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
