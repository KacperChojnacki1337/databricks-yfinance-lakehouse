{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1711359a-4a11-4ae4-904d-92f197216f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.types import IntegerType, FloatType, DateType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# --- 1. Environment Configuration ---\n",
    "try:\n",
    "    # Get environment variable from Databricks widget or set default\n",
    "    ENV = dbutils.widgets.get(\"env_name\")\n",
    "except Exception:\n",
    "    ENV = 'TEST'\n",
    "    \n",
    "# --- 2. Load Configuration ---\n",
    "try:\n",
    "    # Load configuration from the YAML file\n",
    "    with open('../../config/config.yaml', 'r') as file:\n",
    "        full_config = yaml.safe_load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'config.yaml' file not found! Check the path.\")\n",
    "    raise\n",
    "\n",
    "CFG = full_config.get(ENV)\n",
    "if not CFG:\n",
    "    raise ValueError(f\"Configuration not found for environment: {ENV} in YAML file.\")\n",
    "\n",
    "catalog_name = CFG['catalog_name']\n",
    "schema_name = CFG['schema_name']\n",
    "volume_name = CFG['volume_name']\n",
    "\n",
    "# Define the base path for the Bronze table\n",
    "base_output_directory = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_bronze_data\"\n",
    "print(f\"Path to the Bronze Delta Table: {base_output_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e8704c6-e260-4b8a-88a3-aa951275550f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- BRONZE LAYER: Read & Type Optimization ---\n",
    "\n",
    "# Read data from the Bronze Delta table\n",
    "try:\n",
    "    # Use \"delta\" format to read the Delta Lake folder\n",
    "    df_bronze = (\n",
    "        spark.read.format(\"delta\") \n",
    "        .load(base_output_directory) # Load from the Delta folder path\n",
    "        .withColumn(\"Date\", col(\"Date\").cast(DateType()))\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully loaded data from Bronze. Row count: {df_bronze.count()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: An issue occurred while loading data: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed2fe81-7714-453c-948c-4e91a742e40d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply data type optimization\n",
    "df_optimized = (\n",
    "    df_bronze\n",
    "    .withColumn(\"Date\", col(\"Date\").cast(DateType())) # Date: remains 'date'\n",
    "    \n",
    "    # Change from 'double' to 'float' (for smaller memory footprint)\n",
    "    .withColumn(\"Open\", col(\"Open\").cast(FloatType()))\n",
    "    .withColumn(\"High\", col(\"High\").cast(FloatType()))\n",
    "    .withColumn(\"Low\", col(\"Low\").cast(FloatType()))\n",
    "    .withColumn(\"Close\", col(\"Close\").cast(FloatType()))\n",
    "    \n",
    "    # Change from 'long' to 'integer'\n",
    "    .withColumn(\"Volume\", col(\"Volume\").cast(IntegerType())) \n",
    "    # 'Ticket' and 'company_name' remain 'string'\n",
    ")\n",
    "\n",
    "# Replace the original DataFrame with the optimized one\n",
    "df_bronze = df_optimized\n",
    "\n",
    "print(f\"Schema optimized.\")\n",
    "df_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c12bf5d2-fd35-4fe8-9c21-4f39c0c29581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- BRONZE LAYER: Data Quality Checks (Nulls and Duplicates) ---\n",
    "\n",
    "# 1. Minimal Validation for Nulls in Critical Columns\n",
    "critical_null_count = df_bronze.filter(\n",
    "    F.col(\"Date\").isNull() | \n",
    "    F.col(\"Ticket\").isNull() | \n",
    "    F.col(\"Close\").isNull()\n",
    ").count()\n",
    "\n",
    "if critical_null_count > 0:\n",
    "    # Raise an error to stop the ETL pipeline and require intervention\n",
    "    raise ValueError(f\"QA ERROR: Found {critical_null_count} rows with NULLs in critical columns (Date, Ticket, Close). Pipeline stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "696cf23c-c70e-403b-8ec5-2c0fc51c88c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Duplicate Check based on Date & Ticket\n",
    "deduplication_key = [\"Date\", \"Ticket\"]\n",
    "total_rows = df_bronze.count()\n",
    "df_unique = df_bronze.dropDuplicates(subset=deduplication_key)\n",
    "unique_rows = df_unique.count()\n",
    "duplicate_count = total_rows - unique_rows\n",
    "duplicate_percentage = (duplicate_count / total_rows) * 100\n",
    "\n",
    "print(f\"\\n--- Duplicate Report ---\")\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Unique rows: {unique_rows}\")\n",
    "print(f\"Duplicates found: {duplicate_count}\")\n",
    "print(f\"Duplicate percentage: {duplicate_percentage:.2f}%\")\n",
    "\n",
    "# IMPLEMENTATION OF PIPELINE HALT ON DUPLICATES\n",
    "if total_rows != unique_rows:\n",
    "    # Use ValueError to halt the pipeline and flag the error\n",
    "    raise ValueError(\n",
    "        f\"QA ERROR: Duplicates found in key (Date, Ticket)! \"\n",
    "        f\"Total duplicates: {duplicate_count} rows ({duplicate_percentage:.2f}%). \"\n",
    "        f\"ETL pipeline halted for source data inspection.\"\n",
    "    )\n",
    "    \n",
    "# If no duplicates, proceed with the unique DataFrame\n",
    "df_bronze = df_unique \n",
    "print(\"DUPLICATE VALIDATION: SUCCESS. Proceeding to Silver layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f1a603-d523-4642-b9a9-fcc05da7be29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- SILVER LAYER: Feature Engineering & Transformation ---\n",
    "\n",
    "PRECISION = 4 # Use the same precision constant\n",
    "\n",
    "# 1. Calculate Daily Returns (Pct Change)\n",
    "\n",
    "# Define Window: Partition by Ticket, ordered by Date\n",
    "window_spec = (\n",
    "    Window.partitionBy(\"Ticket\")\n",
    "    .orderBy(\"Date\")\n",
    ")\n",
    "\n",
    "df_silver = (\n",
    "    df_bronze\n",
    "    .withColumn(\n",
    "        \"Previous_Close\", \n",
    "        # Calculate the close price from the previous trading day\n",
    "        F.lag(F.col(\"Close\"), 1).over(window_spec)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Daily_Return_Pct\", \n",
    "        # Calculate percentage change and explicitly round to PRECISION places\n",
    "        F.round(\n",
    "            ((F.col(\"Close\") - F.col(\"Previous_Close\")) / F.col(\"Previous_Close\")) * 100, \n",
    "            PRECISION # Use the constant 4\n",
    "        ).cast(FloatType())\n",
    "    )\n",
    "    .drop(\"Previous_Close\") # Remove the temporary column\n",
    ")\n",
    "\n",
    "print(f\"\\nSuccessfully calculated and rounded 'Daily_Return_Pct' to {PRECISION} decimal places.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1107305-0a85-4cb8-b47f-928f85c5e666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Add Time Dimension Columns\n",
    "df_silver = (\n",
    "    df_silver\n",
    "    .withColumn(\n",
    "        \"Year\", \n",
    "        F.year(F.col(\"Date\")) # Extracts year (e.g., 2023)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Quarter\", \n",
    "        F.quarter(F.col(\"Date\")) # Extracts quarter (1, 2, 3, or 4)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Month\", \n",
    "        F.month(F.col(\"Date\")) # Extracts month (1 to 12)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"WeekOfYear\", \n",
    "        # Extracts week number in the year (1 to 53)\n",
    "        F.weekofyear(F.col(\"Date\")) \n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Successfully added Year, Quarter, Month, WeekOfYear columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e77a3507-d18c-4651-a1ee-430ed6aceb99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Calculate Simple Moving Averages (SMA)\n",
    "\n",
    "SMA_PERIODS = [20, 50, 200]\n",
    "base_window_spec = (\n",
    "    Window.partitionBy(\"Ticket\")\n",
    "    .orderBy(\"Date\")\n",
    ")\n",
    "\n",
    "df_silver_sma = df_silver\n",
    "\n",
    "# Iteratively create SMA columns\n",
    "for N in SMA_PERIODS:\n",
    "    # Define Window Specification for period N\n",
    "    # rowsBetween(-(N-1), 0) means: from N-1 rows back up to the current row (0)\n",
    "    window_spec_n = base_window_spec.rowsBetween(-(N - 1), 0)\n",
    "\n",
    "    column_name = f\"SMA_{N}\"\n",
    "    \n",
    "    # Calculate Simple Moving Average (SMA)\n",
    "    df_silver_sma = df_silver_sma.withColumn(\n",
    "        column_name,\n",
    "        # Use avg() function on 'Close' column within the defined window\n",
    "        F.avg(F.col(\"Close\")).over(window_spec_n).cast(FloatType())\n",
    "    )\n",
    "df_silver = df_silver_sma\n",
    "\n",
    "print(f\"Successfully added SMA columns for periods: {SMA_PERIODS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "741499f3-77d4-4cc6-9586-a1bf3de6b962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- SILVER LAYER: Write to Delta Lake ---\n",
    "\n",
    "# Path for Silver layer\n",
    "silver_table_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/yfinance_silver_data\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "try:\n",
    "    dbutils.fs.mkdirs(silver_table_path)\n",
    "    print(f\"Successfully created directory for Silver: {silver_table_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Silver directory already exists or error occurred: {e}\")\n",
    "\n",
    "# Recommended write: overwrite + partitioning\n",
    "(\n",
    "    df_silver.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"Ticket\", \"Year\") # Partitioning by Ticket and Year for query optimization\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .save(silver_table_path)\n",
    ")\n",
    "\n",
    "print(f\"Successfully saved data to Silver layer at: {silver_table_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Yfinance -silver-layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
